openapi: 3.0.0
servers:
  - url: https://llm.aplisay.com/api
info:
  title: LLM Agent API
  version: 1.0.0
  description: |-
    Simple API to create, update and monitor agents connecting inbound telephone calls to 
    a large language model (LLM) based AI engine via Jambonz.

    Allows the LLM parameters such as `prompt` and `temperature` to be set at agent creation time and later modified via updates which affect all future calls.
    Also allows setting and modification of text to speech (TTS) and speech to text (STT) parameters which are used for voice recognition and voicing of responses.
    
    Provides a websocket based `progess` interface which communicates events which take place on calls, allowing the controlling software to capture data and modify
    the prompt in response to client interaction.

    For a demo of the API in action without writing any code, try out the [Aplisay LLM playground](https://llm.aplisay.com), a hosted React based front-end.

    Typical simple transaction flow will be:
      
    1. **[GET models](#operations-Models-modelList)** to obtain a list of valid models.
    2. **[POST agents](#operations-Agent-createAgent)** to create an agent.
    3. Test the return value from the above POST to make sure the agent has been created, a `number` and `socket` have been returned.
    4. Open a new Websocket client to the URL returned in `socket`, and listen on results.
    5. Place calls to the returned `number`, observing the progress updates on the websocket.
    6. When done with the agent, close the websocket client.
    7. Call **[DELETE agents/:{id}](#operations-Agent-deleteAgent)**.

    Note that server may tear down the agent in response terminal error conditions in the telephone interface or LLM service
    and may also do so when you close the websocket client. You may receive `error` events on the websocket and/or the 
    socket may be closed at the server end to indicate a terminal failure. It is good practice to call **DELETE** when you
    detect this or otherwise when done with an agent, but you should tolerate failure of this call as the agent may already
    have been destroyed by the error or in response to your closing the client.

    If you don't set TTS or STT language parameters, then a default will be used for calls to your agent.
    This can be overriden by substituting one of the available values obtained from the list returned by **[GET voices](#operations-Voices-voicesList)**.
components:
  schemas:
    Model:
      type: object
      properties:
        description:
          description: Agent Description
          type: string
          example: GPT3.5-turbo chat
        defaultPrompt:
          description: A working initial default prompt for this agent
          type: string
          example: You are a helpful agent...
      required:
        - name
        - defaultPrompt
    Agent:
      type: object
      properties:
        modelName:
          $ref: '#/components/schemas/ModelName'
        prompt:
          $ref: '#/components/schemas/Prompt'
        options:
          $ref: '#/components/schemas/AgentOptions'
    ModelName:
      type: string
      description: The short model name
      example: gpt35
    Prompt:
      type: string
      description: The prompt to be used in the LLM engine
      example: |-
        You work for Robs Flags, a company that manufactures flags.
        You can only chat with callers about submitting or organising the return of an order that the user has previously made...
    AgentOptions:
      type: object
      properties:
        temperature:
          description: Agent LLM temperature
          type: number
          example: 0.2
        tts:
          type: object
          properties:
            language:
              $ref: '#/components/schemas/Language'
            voice:
              description: |-
                TTS voice specifier.
                Must be a supported voice language as returned from a get on the `voices` api
              type: string
              example: en-GB-Wavenet-A
        stt:
          type: object
          properties:
            language:
              $ref: '#/components/schemas/Language'
    Language:
      description: >-
        Language and country dialect specified as an ISO639-1 language code
        followed by a dash and and ISO3166 country code.
        For now, list of supported recognition voices is identical to the voicing languages returned from the `voices` api.
        This should change in future
      type: string
      example:
        - ca-ES
        - en-GB
        - en-IN
    Voice:
      type: object
      properties:
        name:
          type: string
          description: The voice name or identifier within the TTS engine - opaque string
          example: en-GB-Wavenet-B
        gender:
          type: string
          description: The vendor assigned gender of this voice within the TTS engine
          example: male
    Error:
      type: object
      properties:
        code:
          type: string
          description: String representation of a numeric error code
        message:
          type: string
          description: Human readable error condition (largely) suitable to present to the client.
paths: {}
